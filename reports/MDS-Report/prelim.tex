\subsection{Linear codes}
\begin{definition}[Hamming weight \cite{SloaneBook}]
The Hamming weight $w(x)$ of a vector $x$ is the number of nonzero components of the vector $x$.
\end{definition}

\begin{definition}[Hamming distance \cite{SloaneBook}]
The Hamming distance between two vectors $x$ and $y$ is $w(x - y)$, which is equal to the Hamming weight of the difference of the two vectors.
\end{definition}

\begin{definition}[Linear code \cite{SloaneBook}]
A linear $[n, k, d]$ code over a field $\mathbb{F}$ is a $k$-dimensional subspace of the vector space $\mathbb{F}^n$, where any two different vectors of the subspace have a Hamming distance of at least $d$, and $d$ is the largest number with this property.

The distance $d$ of a linear code equals the minimum weight of a non-zero codeword in the code. A linear code can be described by generator and/or parity-check matrices.
\end{definition}

\begin{definition}[Generator matrix \cite{SloaneBook}]
A generator matrix $G$ for an $[n, k, d]$ code $C$ is a $k \times n$ matrix whose rows form a vector space basis for $C$. The choice of a basis in a vector space is not unique, thus a code has many different generator matrices which can be reduced to one another by performing elementary row operations.
\end{definition}

\begin{definition}[Parity-check matrix \cite{SloaneBook}]
A parity-check matrix $H$ for an $[n, k, d]$ code $C$ is an $(n-k) \times k$ matrix with the property that a vector $x$ is a codeword of $C$ iff $Hx^T = 0$.
\end{definition}

\begin{theorem}[Singleton Bound \cite{SloaneBook}]
If $C$ is an $[n, k, d]$ code, then $d \leq n - k + 1$.
\end{theorem}

\begin{definition}[MDS code \cite{SloaneBook}]\label{def:mds-code-singleton}
An \emph{MDS code} is a \emph{linear code} that meets the \emph{Singleton bound}, i.e a linear code with $d = n - k + 1$.
\end{definition}

An MDS matrix associated to a $[n, k, d]$ code has \emph{branch number} equal to $d$, which is the maximum possible branch number, thus providing optimal diffusion.

\begin{definition}[Branch number \cite{Shirai2003}]
The branch number $\mathcal{B}$ of a linear mapping $\theta: GF(2^m)^k \rightarrow GF(2^m)^l$ is defined as:

$$\mathcal{B}(\theta) = \min_{a\neq0}\{w(a) + w(\theta(a))\}.$$

Note that here, $+$ denotes a XOR operation, since the fields have characteristic equal to 2.

Furthermore, given a matrix $M$, associated to a $[n, k, d]$ linear code, the linear mapping defined by $\theta(a) = a \cdot M$ has branch number $\mathcal{B}(\theta) = d$. When the linear code is MDS, i.e when $M$ is MDS, the mapping is an optimal diffusion mapping.
\end{definition}

\subsection{Matrices}

\textcolor{red}{Obs: eu escrevi essas primeiras definições (matriz singular, involutória, circulante, circulante à esquerda, circulante à direita) com base no que eu lembrava de matemática mesmo, então por hora ainda não coloquei uma referência bibliográfica, já que são definições mais gerais e não chegam a ser específicas de cripto. Mas posso colocar depois se necessário.}

\begin{definition}[Singular matrix]
A square matrix $A$ is singular if and only if $det(A) = 0$.
\end{definition}

\begin{definition}[Non-singular matrix]
$A$ is non-singular if and only if $det(A) \neq 0$.
\end{definition}

\begin{definition}[Involutory matrix]
An $n \times n$ square matrix $A$ is involutory if $A \times A = I_n$, where $I_n$ is the identity matrix. In other words, $A$ is involutory when $A = A^{-1}$.
\end{definition}

\begin{definition}[Circulant matrix]
An $n \times n$ matrix $A$ is circulant if each row $i$ is formed by a cyclical shift of $i$ positions of the same set of elements $\{a_0, a_1, a_2, ..., a_{n-1}\}$.
\end{definition}

\begin{definition}[Left circulant matrix]
A circulant matrix in which the shift is a cyclical shift to the left, i.e

$$
A =
\begin{bmatrix}
a_0 & a_1 & ... & ... & a_{n-1}\\
a_1 & a_2 & ... & a_{n-1} & a_0\\
... & ... & ... & ... & ...\\
a_{n-1} & a_0 & ... & ... & a_{n-2}
\end{bmatrix}.
$$
\end{definition}

\begin{definition}[Right circulant matrix]
A circulant matrix in which the shift is a cyclical shift to the right, i.e

$$
A =
\begin{bmatrix}
a_0 & a_1 & ... & ... & a_{n-1}\\
a_{n-1} & a_0 & a_1 & ... & a_{n-2}\\
... & ... & ... & ... & ...\\
a_1 & ... & a_{n-2} & a_{n-1} & a_0
\end{bmatrix}.
$$
\end{definition}

Note that circulant matrices can be defined by just one row, since all the other rows are cyclical shifts of the first. Therefore, they can be denoted as $circ(a_0, a_1, ..., a_{n-1})$. In the case of left circulant and right circulant matrices, respectively, $lcirc(a_0, ..., a_{n-1})$ and $rcirc(a_0, ..., a_{n-1})$. For example, matrices (\ref{mat:rijndael}) and its inverse (\ref{mat:rijndael}), used in the Rijndael cipher, can be denoted as $rcirc(02_x, 03_x, 01_x, 01_x)$ and $rcirc(0e_x, 0b_x, 0d_x, 09_x)$.

\begin{definition}[Transpose matrix]
The transpose of a matrix $A$, denoted by $A^T$, is the matrix such that $A^T[i][j] = A[j][i]$. In other words, it is the matrix obtained by writing the rows of $A$ as columns. Note that, if $A$ is an $m \times n$ matrix, then $A^T$ will be $n \times m$.
\end{definition}

\begin{definition}[Submatrix]
Given a matrix $M$, a submatrix of $M$ is the matrix obtained after removing $z$ rows and columns of $M$, $z \geq 1$, provided that there are sufficient rows (and columns) to be removed.
\end{definition}

\begin{theorem}[MDS codes \cite{SloaneBook}]\label{teo:mds}
An $[n, k, d]$-code $\mathcal{C}$ with generator matrix $G = [I_nB]$, where $B$ is a $k \times (n - k)$ matrix, is MDS if and only if every square submatrix of $B$ is non-singular.
\end{theorem}

We call the $B$ matrix of Theorem \ref{teo:mds} the \emph{MDS matrix} throughout this work, i.e the MDS matrices we study are the $B$ matrices of the respective MDS codes chosen when designing them.
Note that Definition \ref{def:mds-code-singleton} establishes the conditions for a code to be MDS, therefore Theorem \ref{teo:mds} is an alternative way of evaluating a code, or a matrix, with respect to the MDS property.
For further detail on matrices, determinants and linear algebra, the reader may refer to \cite{LangeLinearAlgebra}.

\begin{definition}[Cauchy matrix]\label{def:cauchy}
Given $x_0, ..., x_{n-1}$ and $y_0, ..., y_{n-1}$, the matrix $A$ where $A[i][j] = \frac{1}{x_i + y_j}$ is called a Cauchy matrix. According to \cite{Youssef1997}, provided that $x_i \neq x_j$ for $0\leq i,j\leq n-1$, that $y_i \neq y_j$ for $0\leq i,j\leq n-1$ and that $x_i + y_j \neq 0$ for all $i, j$, any square submatrix of a Cauchy matrix is nonsingular over any field.
\end{definition}

An $n\times n$ matrix possesses $n^2$ elements and thus, when constructing one e.g by selecting random elements, $n^2$ choices must be made. However, Cauchy and circulant matrices allow us to lower the number of elements we need to select. A circulant matrix can be defined by one row only, therefore only $n$ elements are required. However, there are no guarantees about the MDS property. One must check whether Theorem \ref{teo:mds} holds to ensure the obtained matrix is MDS. On the other hand, a Cauchy matrix construction directly ensures the MDS property, as can be seen in Definition \ref{def:cauchy}, requiring $2n$ choices of elements (the $x_i$ and the $y_i$) to be defined. Furthermore, it is relevant to note that, albeit most matrices in this work have their dimension $n$ be a power of two such as 4 or 8, this is not a requirement for the construction. One can construct $n \times n$ circulant (or Cauchy) matrices for any arbitrary $n$.

\begin{definition}[Hadamard matrix \cite{beauchamp1975walsh}]\label{def:hadamard}
Given $n$ elements $a_0, a_1, ..., a_{n-1}$, $n$ being a power of 2, the matrix $H$ such that $H[i][j] = a_{i \oplus j}$ is a Hadamard matrix.
\end{definition}

\begin{definition}[Vandermonde matrix \cite{hoffmann1971linear}]
Given $z$ elements $a_0, a_1, ..., a_{z-1}$, the $z \times n$ matrix $V$ such that
\begin{equation}\label{eq:cost}
V =
\begin{bmatrix}
1 & a_0 & a_0^2 & ... & a_0^{n-1}\\
1 & a_1 & a_1^2 & ... & a_1^{n-1}\\
... & ... & ... & ... & ...\\
1 & a_{z-1} & a_{z-1}^2 & ... & a_{z-1}^{n-1}
\end{bmatrix}
\end{equation}
is a Vandermonde matrix.
\end{definition}

Note that Hadamard and Vandermonde constructions too allow us to lower the number of required choices to obtain a matrix. Only $n$ elements are necessary for a Hadamard matrix to be defined, provided that $n$ is a power of 2. In the case of the Vandermonde matrix, $z$ elements are required and then we must choose the number of columns $n$. Not all Vandermonde matrices are square matrices, but the notion of MDS matrix applies only to square matrices. Therefore, $z$ must be equal to $n$ if we wish to construct a Vandermonde MDS matrix. There are no guarantees about the MDS property with Hadamard or Vandermonde constructions, so we must construct the matrices and then evaluate whether they are MDS or not, e.g using Theorem \ref{teo:mds}.

In Whirlwind\cite{Whirlwind2010}, the authors make use of \emph{dyadic} matrices, defining them as a matrix $S$ such that $S[i][j] = s_{i\oplus j}$ given a set $s_0, s_1, ... , s_{n-1}$ of $n$ elements. This definition matches Definition \ref{def:hadamard}, which is used in Anubis and Khazad, but referred as Hadamard instead of dyadic.

\subsection{Evaluating a matrix for MDS property}
If Theorem \ref{teo:mds} is used, the determinant of the matrix itself must be calculated, as well as the determinants of its submatrices. First, we derive a formula for the cost of calculating the determinant of an $n \times n$ matrix. Our cost unit is \emph{the number of multiplications in a finite field}, since it is the most expensive operation.

The determinant of a  $2 \times 2$ matrix $\begin{bmatrix}a & b \\ c & d\end{bmatrix}$ can be computed by means of the formula $ad - bc$, requiring therefore two multiplications.

The determinant of a $3 \times 3$ matrix can be computed by calculating cofactors of $2 \times 2$ submatrices obtained by removing a row $i$ and a column $j$. For example, the determinant of $\begin{bmatrix}a & b & c\\d & e & f\\g & h & i\end{bmatrix}$ can be computed as $a \begin{vmatrix}e & f\\h & i \end{vmatrix} + b \begin{vmatrix}d & f\\ g & i\end{vmatrix} + c \begin{vmatrix}d & e\\g & h\end{vmatrix}$, where $\begin{vmatrix}e & f\\h & i \end{vmatrix}$ denotes the determinant of the submatrix $\begin{bmatrix}e & f\\h & i \end{bmatrix}$.

The cost is therefore $3 + 3t(2)$, where $t(2)$ is the number of multiplications required to compute the determinant of a $2\times2$ matrix. We know that $t(2) = 2$, therefore the cost to obtain the determinant of a $3\times3$ matrix is $3+3\times2 = 9$, i.e $t(3) = 9$. Extending this logic, let $t(n)$ be the cost of obtaining the determinant of an $n\times n$ matrix. Then $t(n) = n + nt(n-1) = n + n(n-1 + (n-1) t(n-2)) = n + n(n-1) + n(n-1)t(n-2) = n + n(n-1) + n(n-1)(n-2) + ... + n(n-1)(n-2)..2$. For example, for $n = 4$, $t(4) = 4 + 4\times3 + 4\times3\times2 = 4 + 12 + 24 = 40$ multiplications in a finite field.

In order to evaluate a matrix for MDS property, we must compute the determinants of all its square submatrices. Let $A$ be an $n \times n$ matrix. A submatrix of $A$ is obtained by removing $z$ rows and columns of $A$. The possible values of $z$ range from $1$ to $n - 1$. When removing $n-1$ rows and columns though, only single matrix elements remain. We evaluate if they are zero or not, and this does not require multiplications in the underlying finite field, only equality checks. When removing $z$ rows and columns, $(n - z) \times (n - z)$ submatrices are obtained, and computing a determinant costs $t(n-z)$ multiplications in a finite field. There are $2^z$ ways of choosing which rows are to be removed, and $2^z$ ways of choosing which columns are to be removed. Therefore, $2^z \times 2^z = 2^{2z}$ possible $(n - z) \times (n - z)$ submatrices, totalizing $2^{2z}$ determinants to evaluate. The cost is therefore $2^{2z}\times t(n-z)$ multiplications for a given $z$. Let $t'(z) = 2^{2z} \times t(n-z)$. Since $z$ ranges from $1$ to $n-2$, the total cost to evaluate a matrix for MDS property will be $t'(1) + t(2) + ... + t'(n-2)$.

\subsection{Abstract algebra}
\textcolor{red}{Aqui pretendo colocar definições de grupo, grupo abeliano, corpo etc. A parte de álgebra abstrata que não é específica de corpos finitos e que a gente geralmente vê na faculdade}

\subsection{Finite fields --- GF($2^m$)}

\begin{concept}[Finite field \cite{DesignOfRijndael2002}]
A \emph{finite field} is a field with a finite number of elements. The number of elements in the set is called the \emph{order} of the field.
\end{concept}

\begin{concept}[Characteristic and order \cite{DesignOfRijndael2002}]
A field with order $r$ exists if and only if $r$ is a prime power, i.e $r = p^m$ for some integer $m$, where $p$ is a prime integer. $p$ is called the \emph{characteristic} of the field. For each prime power there is exactly one finite field, denoted by GF($p^m$).
\end{concept}

\begin{concept}[Representing finite fields with prime order \cite{DesignOfRijndael2002}]
Elements of a finite field GF($p$) can be represented by the integers $0, 1, ..., p - 1$, and the field operations are integer addition modulo $p$ and integer multiplication modulo $p$.
\end{concept}

\begin{concept}[Representing finite fields with non-prime order \cite{DesignOfRijndael2002}]
For finite fields with an order $r = p^m$ that is not prime, addition and multiplication cannot be represented by addition and multiplication modulo $r$, and instead other representations must be used. One of the possible representations for GF($p^m$) is by means of \emph{polynomials over GF($p$) with degree at most $m-1$}. Addition and multiplication are then defined modulo an \emph{irreducible polynomial of degree $m$}.
\end{concept}

\begin{concept}[Polynomial \cite{DesignOfRijndael2002}]
A polynomial over a field $\mathbb{F}$ is an expression of the form

$$
b(x) = b_{m-1}x^{m-1} + b_{m-2}x^{m-2} + ... + b_2x^2 + b_1x + b_0,
$$

where $x$ is the \emph{indeterminate} and $b_i \in \mathbb{F}$ are the coefficients. The \emph{degree} of the polynomial equals $l$ if $b_j = 0$ for all $j > l$ and $l$ is the smallest number with this property.
\end{concept}

Note that, when using polynomials over GF($p$) to represent the GF($p^m$) field, the degree is at most $m-1$.

In this chapter, we focus particularly on fields with characteristic $p = 2$, due to their wide application in cryptography.

Addition and multiplication are defined on polynomials as follows.

\begin{concept}[Polynomial addition \cite{DesignOfRijndael2002}]
Summing two polynomials $a(x)$ and $b(x)$ consists of summing the coefficients with equal powers of $x$, with the sum occuring in the underlying field $\mathbb{F}$. The neutral element is 0 (the polynomial with all coefficients equal to 0). The inverse element can be found by replacing each coefficient by its inverse element in $\mathbb{F}$. The degree of $a(x) + b(x)$ is at most the maximum of the degrees of $a(x)$ and $b(x)$, therefore addition is closed.
\end{concept}

For polynomials over GF($2$) stored as integers in a cryptographic software implementation, addition can be implemented with a bitwise XOR instruction.

\begin{concept}[Polynomial multiplication \cite{DesignOfRijndael2002}]
In order to make multiplication closed, we select a polynomial $p(x)$ of degree $l$, called the \emph{reduction polynomial}. Multiplication of $a(x)$ and $b(x)$ is then defined as the algebraic product of the polynomials modulo the reduction polynomial $p(x)$.

The neutral element is 1 (the polynomial of degree 0 and with coefficient of $x^0$ equal to 1). The inverse element of $a(x)$ is $a^{-1}(x)$ such that $a(x) \times a^{-1}(x) = 1 \text{mod} p(x)$. Note that $a^{-1}(x)$ exists only when $a(x) \neq 0$.
\end{concept}

For polynomials over GF($2$) stored as integers in a cryptographic software implementation, multiplication by $x$ can be implemented as a logical bit shift followed by conditional XOR (i.e subtraction) of the reduction polynomial (the \textbf{xtime} operation). Multiplication by other polynomials can be implemented as a series of \textbf{xtime}.

The reduction polynomial is chosen as an irreducible polynomial.

\begin{concept}[Irreducible polynomial \cite{DesignOfRijndael2002}]
A polynomial $d(x)$ is irreducible over the field GF($p$) if and only if there exist no two polynomials $a(x)$ and $b(x)$ with coefficients in GF($p$) such that $d(x) = a(x) \times b(x)$, where $a(x)$ and $b(x)$ are of degree greater than 0.
\end{concept}

For further reference on abstract algebra and Finite Fields, the reader may refer to \cite{panario2007topicos}, \cite{panario2013handbook} and \cite{Handbook1996}.

\subsection{Computational cost unit}\label{sec:comp-cost}

\subsubsection{Computational cost of multiplication in GF($2^8$)}
Consider $T$ a state byte, which we multiply by the polynomial $2e_x = 00101110_2 = x^5 + x^3 + x^2 + x$ in GF($2^8$). Note that

$$
T \cdot 2e_x = T \cdot x^5 + T \cdot x^3 + T \cdot x^2 + T \cdot x = T \cdot x \cdot x \cdot x \cdot x \cdot x + T \cdot x \cdot x \cdot x + T \cdot x \cdot x + T \cdot x,
$$

where $\cdot$ denotes multiplication and $+$ denotes addition (which, in GF($2^8$), is equivalent to a bitwise XOR). Multiplication by the $x$ polynomial is performed by \textbf{xtime}, and addition is performed by \textbf{xor}.

Let $T \cdot x = Y$. Then $T \cdot 2e_x = Y + Y \cdot x + Y \cdot x \cdot x + Y \cdot x \cdot x \cdot x \cdot $.

Let $Y \cdot x = W$. Then $T \cdot 2e_x = Y + W + W \cdot x + W \cdot x \cdot x \cdot x$.

Let $W \cdot x = Z$. Then $T \cdot 2e_x = Y + W + Z + Z \cdot x \cdot x$.

The total number of \textbf{xtime} operations in this process is 5 (1 to obtain $Y$ from $T$, 1 to obtain $W$ from $Y$, 1 to obtain $Z$ from $W$, 2 to compute $Z \cdot x \cdot x$), since we can reuse intermediate \textbf{xtime} calls. The total number of \textbf{xor} operations is 3. For multiplication in GF($2^8$), in the worst case, 7 \textbf{xtime} would be necessary, since the maximum degree of polynomials in GF($2^8$) is 7.

\subsubsection{Computational cost of a matrix}
The computational cost of an $n\times n$ matrix $A$ is given by the necessary \textbf{xor} and \textbf{xtime} operations when multiplying a $n\times1$ column vector by $A$. As an example, we calculate the cost of matrix (\ref{mat:square}), used in the SQUARE \cite{SQUARE1997} cipher.

A row of matrix (\ref{mat:square}) contains the elements $01_x = 1, 02_x = x$ and $03_x = x + 1$ only. Multiplying by $01_x$ does not require \textbf{xtime} or \textbf{xor}, since $01_x \cdot T = T$. Computing $02_x \cdot T = x \cdot T$ requires 1 \textbf{xtime}. Computing $03_x \cdot T = (x + 1) \cdot T = T \cdot x + T$ requires 1 \textbf{xtime} and 1 \textbf{xor}. Furthermore, adding the row multiplication results costs 3 \textbf{xor}. Therefore, the cost of a row is 2 \textbf{xtime} and 4 \textbf{xor}. Equation \ref{eq:cost} illustrates this, with $t_1, t_2, t_3$ and $t_4$ being bytes of the state column vector.

\begin{equation}\label{eq:cost}
\begin{bmatrix}
02_x & 01_x & 01_x & 03_x\\
\end{bmatrix}
\cdot
\begin{bmatrix}
t_1\\
t_2\\
t_3\\
t_4
\end{bmatrix}
= 02_x \cdot t_1 + 01_x \cdot t_2 + 01_x \cdot t_3 + 03_x \cdot t_4
\end{equation}

Note that matrix (\ref{mat:square}) contains 4 rows, yielding a total cost of 8 \textbf{xtime} and 16 \textbf{xor}.
